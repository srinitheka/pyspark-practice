{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7231719926451859456/\n",
    "# SQL Interview Questions:\n",
    "# ğƒğ¢ğŸğŸğ¢ğœğ®ğ¥ğ­ğ² - ğŒğğğ¢ğ®ğ¦\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType, DateType\n",
    "from pyspark.sql import Window as W\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SQL Interview Questions\") \\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Schema : Using Employees (EmployeeID, Name, RegionID) and Sales (SaleID, EmployeeID, Amount)\n",
    "employees_data = [\n",
    "    (\"E101\", \"John Doe\", \"R001\"),\n",
    "    (\"E102\", \"Jane Smith\", \"R002\"),\n",
    "    (\"E103\", \"Mike Johnson\", \"R001\"),\n",
    "    (\"E104\", \"Emily Davis\", \"R003\"),\n",
    "    (\"E105\", \"Sarah Brown\", \"R001\"),\n",
    "    (\"E106\", \"Michelle Ramirez\", \"R002\"),\n",
    "    (\"E107\", \"Michael Thompson\", \"R003\"),\n",
    "    (\"E108\", \"Jessica Taylor\", \"R004\"),\n",
    "    (\"E109\", \"Daniel Anderson\", \"R002\"),\n",
    "    (\"E110\", \"Laura Martinez\", \"R003\"),\n",
    "    (\"E111\", \"Christopher Lee\", \"R001\"),\n",
    "    (\"E112\", \"Anthony Harris\", \"R004\"),\n",
    "    (\"E113\", \"Patricia Adams\", \"R005\"),\n",
    "    (\"E114\", \"Jennifer Clark\", \"R005\"),\n",
    "    (\"E115\", \"Robert Young\", \"R004\")\n",
    "]\n",
    "\n",
    "# Create DataFrame for Employees\n",
    "employees_df = spark.createDataFrame(employees_data, [\"EmployeeID\", \"Name\", \"RegionID\"])\n",
    "\n",
    "# Sample data for Sales\n",
    "sales_data = [\n",
    "    (\"S001\", \"E101\", 5000),\n",
    "    (\"S002\", \"E102\", 3000),\n",
    "    (\"S003\", \"E103\", 4000),\n",
    "    (\"S004\", \"E104\", 2000),\n",
    "    (\"S005\", \"E105\", 3500),\n",
    "    (\"S006\", \"E106\", 4500),\n",
    "    (\"S007\", \"E107\", 3000),\n",
    "    (\"S008\", \"E108\", 6000),\n",
    "    (\"S009\", \"E109\", 7000),\n",
    "    (\"S010\", \"E110\", 8000),\n",
    "    (\"S011\", \"E111\", 2500),\n",
    "    (\"S012\", \"E112\", 4000),\n",
    "    (\"S013\", \"E113\", 1500),\n",
    "    (\"S014\", \"E114\", 3000),\n",
    "    (\"S015\", \"E115\", 5000)\n",
    "]\n",
    "\n",
    "# Create DataFrame for Sales\n",
    "sales_df = spark.createDataFrame(sales_data, [\"SaleID\", \"EmployeeID\", \"Amount\"])\n",
    "employees_df.createOrReplaceTempView(\"Employees\")\n",
    "sales_df.createOrReplaceTempView(\"Sales\")\n",
    "\n",
    "# Find the Top 3 Employees with the Highest Total Sales in Each Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Schema :  Sales (ProductID, SaleDate, Quantity).\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "from datetime import date\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SalesDataWithIncreasingTrends\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the schema for Sales\n",
    "sales_schema = StructType([\n",
    "    StructField(\"ProductID\", StringType(), True),\n",
    "    StructField(\"SaleDate\", DateType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Sample data for Sales (with increasing sales across three months)\n",
    "sales_data = [\n",
    "    (\"P001\", date(2024, 7, 1), 100),\n",
    "    (\"P001\", date(2024, 8, 1), 150),\n",
    "    (\"P001\", date(2024, 9, 1), 200),\n",
    "    (\"P002\", date(2024, 7, 2), 80),\n",
    "    (\"P002\", date(2024, 8, 2), 130),\n",
    "    (\"P002\", date(2024, 9, 2), 180),\n",
    "    (\"P003\", date(2024, 7, 3), 90),\n",
    "    (\"P003\", date(2024, 8, 3), 140),\n",
    "    (\"P003\", date(2024, 9, 3), 190),\n",
    "    (\"P004\", date(2024, 7, 4), 70),\n",
    "    (\"P004\", date(2024, 8, 4), 100),\n",
    "    (\"P004\", date(2024, 9, 4), 130),\n",
    "    (\"P005\", date(2024, 7, 5), 110),\n",
    "    (\"P005\", date(2024, 8, 5), 140),\n",
    "    (\"P005\", date(2024, 9, 5), 170),\n",
    "    (\"P006\", date(2024, 7, 6), 60),\n",
    "    (\"P006\", date(2024, 8, 6), 110),\n",
    "    (\"P006\", date(2024, 9, 6), 160)\n",
    "]\n",
    "\n",
    "# Create DataFrame for Sales\n",
    "sales_df = spark.createDataFrame(sales_data, sales_schema)\n",
    "sales_df.createOrReplaceTempView(\"sales\")\n",
    "# Identify Products with Sales Increasing for Three Consecutive Months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema : Orders (OrderID, CustomerID, OrderDate)\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType\n",
    "from datetime import date\n",
    "\n",
    "# Define the schema for Orders\n",
    "orders_schema = StructType([\n",
    "    StructField(\"OrderID\", StringType(), True),\n",
    "    StructField(\"CustomerID\", StringType(), True),\n",
    "    StructField(\"OrderDate\", DateType(), True)\n",
    "])\n",
    "\n",
    "# Sample data for Orders (with orders placed in consecutive months)\n",
    "orders_data = [\n",
    "    (\"O001\", \"C001\", date(2024, 7, 10)),\n",
    "    (\"O002\", \"C001\", date(2024, 8, 12)),\n",
    "    (\"O003\", \"C001\", date(2024, 9, 15)),\n",
    "    (\"O004\", \"C002\", date(2024, 7, 20)),\n",
    "    (\"O005\", \"C002\", date(2024, 8, 25)),\n",
    "    (\"O006\", \"C003\", date(2024, 6, 5)),\n",
    "    (\"O007\", \"C003\", date(2024, 7, 7)),\n",
    "    (\"O008\", \"C003\", date(2024, 8, 9)),\n",
    "    (\"O009\", \"C004\", date(2024, 5, 13)),\n",
    "    (\"O010\", \"C004\", date(2024, 6, 14)),\n",
    "    (\"O011\", \"C004\", date(2024, 7, 16)),\n",
    "    (\"O012\", \"C005\", date(2024, 7, 22)),\n",
    "    (\"O013\", \"C005\", date(2024, 9, 23)),\n",
    "    (\"O014\", \"C006\", date(2024, 8, 3)),\n",
    "    (\"O015\", \"C006\", date(2024, 9, 4)),\n",
    "    (\"O016\", \"C007\", date(2024, 7, 30)),\n",
    "    (\"O017\", \"C007\", date(2024, 8, 31)),\n",
    "    (\"O018\", \"C007\", date(2024, 9, 1)),\n",
    "]\n",
    "\n",
    "# Create DataFrame for Orders\n",
    "orders_df = spark.createDataFrame(orders_data, orders_schema)\n",
    "orders_df.createOrReplaceTempView(\"Orders\")\n",
    "\n",
    "# List Customers Who Placed Orders in Consecutive Months\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Schema: Employees (EmployeeID, Name, DepartmentID, Salary)\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "\n",
    "# Define the schema for Employees\n",
    "employees_schema = StructType([\n",
    "    StructField(\"EmployeeID\", StringType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"DepartmentID\", StringType(), True),\n",
    "    StructField(\"Salary\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Sample data for Employees (with varying salaries within each department)\n",
    "employees_data = [\n",
    "    (\"E001\", \"John Doe\", \"D001\", 75000.0),\n",
    "    (\"E002\", \"Jane Smith\", \"D001\", 85000.0),\n",
    "    (\"E003\", \"Mike Johnson\", \"D001\", 95000.0),\n",
    "    (\"E004\", \"Emily Davis\", \"D001\", 65000.0),\n",
    "    (\"E005\", \"Sarah Brown\", \"D002\", 72000.0),\n",
    "    (\"E006\", \"Michelle Ramirez\", \"D002\", 82000.0),\n",
    "    (\"E007\", \"Michael Thompson\", \"D002\", 92000.0),\n",
    "    (\"E008\", \"Jessica Taylor\", \"D002\", 62000.0),\n",
    "    (\"E009\", \"Daniel Anderson\", \"D003\", 87000.0),\n",
    "    (\"E010\", \"Laura Martinez\", \"D003\", 97000.0),\n",
    "    (\"E011\", \"Christopher Lee\", \"D003\", 77000.0),\n",
    "    (\"E012\", \"David Wilson\", \"D003\", 67000.0),\n",
    "    (\"E013\", \"Kevin Brown\", \"D004\", 80000.0),\n",
    "    (\"E014\", \"Nancy Green\", \"D004\", 70000.0),\n",
    "    (\"E015\", \"Paul White\", \"D004\", 90000.0),\n",
    "    (\"E016\", \"Laura Red\", \"D004\", 60000.0)\n",
    "]\n",
    "\n",
    "# Create DataFrame for Employees\n",
    "employees_df = spark.createDataFrame(employees_data, employees_schema)\n",
    "employees_df.createOrReplaceTempView(\"Employees\")\n",
    "\n",
    "# Find the Second Highest Salary in Each Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Schema : Given Sales (SaleID, ProductID, SaleDate, Amount)\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, FloatType\n",
    "from datetime import date\n",
    "\n",
    "# Define the schema for Sales\n",
    "sales_schema = StructType([\n",
    "    StructField(\"SaleID\", StringType(), True),\n",
    "    StructField(\"ProductID\", StringType(), True),\n",
    "    StructField(\"SaleDate\", DateType(), True),\n",
    "    StructField(\"Amount\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Sample data for Sales (with sales over a span of months)\n",
    "sales_data = [\n",
    "    (\"S001\", \"P001\", date(2024, 1, 15), 200.0),\n",
    "    (\"S002\", \"P001\", date(2024, 2, 10), 250.0),\n",
    "    (\"S003\", \"P001\", date(2024, 3, 12), 300.0),\n",
    "    (\"S004\", \"P001\", date(2024, 4, 15), 220.0),\n",
    "    (\"S005\", \"P002\", date(2024, 1, 20), 150.0),\n",
    "    (\"S006\", \"P002\", date(2024, 2, 25), 180.0),\n",
    "    (\"S007\", \"P002\", date(2024, 3, 30), 210.0),\n",
    "    (\"S008\", \"P002\", date(2024, 4, 10), 160.0),\n",
    "    (\"S009\", \"P003\", date(2024, 1, 5), 300.0),\n",
    "    (\"S010\", \"P003\", date(2024, 2, 8), 350.0),\n",
    "    (\"S011\", \"P003\", date(2024, 3, 15), 400.0),\n",
    "    (\"S012\", \"P003\", date(2024, 4, 12), 320.0),\n",
    "    (\"S013\", \"P004\", date(2024, 1, 18), 100.0),\n",
    "    (\"S014\", \"P004\", date(2024, 2, 22), 120.0),\n",
    "    (\"S015\", \"P004\", date(2024, 3, 25), 130.0),\n",
    "    (\"S016\", \"P004\", date(2024, 4, 20), 110.0),\n",
    "]\n",
    "\n",
    "# Create DataFrame for Sales\n",
    "sales_df = spark.createDataFrame(sales_data, sales_schema)\n",
    "sales_df.createOrReplaceTempView(\"Sales\")\n",
    "\n",
    "# Calculate the Moving Average of Sales for Each Product Over the Last 3 Months\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# You have a table named 'employee_performance' with columns (employee_id, review_date, performance_score). Write a query to calculate the average performance score for each employee over their last three reviews.\n",
    "\n",
    "\n",
    "# Given a table 'website_traffic' with columns (visit_date, page_views, unique_visitors), write a query to calculate the percentage change in unique visitors from the previous day to the current day.\n",
    "\n",
    "\n",
    "# You have two tables: 'orders' with columns (order_id, order_date, customer_id, order_total) and 'customers' with columns (customer_id, customer_name, signup_date). Write a query to find the total order amount for each customer who has made purchases in the last 6 months, along with their signup date.\n",
    "\n",
    "\n",
    "# Using a table 'inventory' with columns (product_id, quantity_in_stock, reorder_level), write a query to identify products that need to be reordered, along with the percentage of stock remaining based on the reorder level.\n",
    "\n",
    "\n",
    "# Given a table 'user_activity' with columns (user_id, activity_date, activity_type), write a query to determine the most common activity type for each user over the last month, along with the count of that activity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ğ†ğ¨ğ¨ğ ğ¥ğ'ğ¬ ğ’ğğ‹ ğ¢ğ§ğ­ğğ«ğ¯ğ¢ğğ° ğğ®ğğ¬ğ­ğ¢ğ¨ğ§ !\n",
    "\n",
    "\n",
    "# For each unique user in the dataset, find the latest date when their flags got reviewed. Then, find total number of distinct videos that were removed on that date (by any user).\n",
    "\n",
    "# Output the the first and last name of the user (in two columns), the date and the number of removed videos. Only include these users who had at least one of their flags reviewed by Youtube. If no videos got removed on a certain date, output 0.\n",
    "\n",
    "# Tables: \n",
    "\n",
    "# user_flags\n",
    "\n",
    "# user_firstname : varchar\n",
    "# user_lastname : varchar\n",
    "# video_id : varchar \n",
    "# flag_id : varchar\n",
    "\n",
    "# flag_review\n",
    "\n",
    "# flag_id : varchar\n",
    "# reviewed_by_yt : bool\n",
    "# reviewed_date : datetime\n",
    "# reviewed_outcome : varchar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the question is :\n",
    "\n",
    "# Suppose we have a cluster of 10 Nodes,16 cores per Node and \n",
    "# 64GB RAM per Node. How are you going to decide Distribution of Executors, Cores and Memory for a Spark Application execution :\n",
    "\n",
    "# Letâ€™s assign 5 core per executors => --executor-cores = 5 (for good HDFS throughput)\n",
    "\n",
    "# Leave 1 core per node for Hadoop/Yarn daemons => Num cores available per node = 16-1 = 15\n",
    "# So, Total available of cores in cluster = 15 x 10 = 150\n",
    "\n",
    "# Number of available executors = (total cores/num-cores-per-executor) = 150/5 = 30\n",
    "\n",
    "# Leaving 1 executor for ApplicationManager => --num-executors = 29\n",
    "# Number of executors per node = 30/10 = 3\n",
    "\n",
    "# Memory per executor = 64GB/3 = 21GB\n",
    "# Counting off heap overhead = 7% of 21GB = 3GB. \n",
    "\n",
    "# So, actual --executor-memory = 21 - 3 = 18GB\n",
    "\n",
    "# So, recommended config is: 29 executors, 18GB memory each and 5 cores each!!\n",
    "\n",
    "# *You can keep the number of cores per executor in the range of 2-5*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deloitte\n",
    "# ğ’ğğ‹-ğ«ğğ¥ğšğ­ğğ ğğ®ğğ¬ğ­ğ¢ğ¨ğ§ğ¬:\n",
    "\n",
    "# 1. Given a table named \"financial_transaction\" with columns like transaction_id, transaction_date, amount, and transaction_type (credit or debit), \n",
    "# write an SQL query to calculate the total balance for each day, considering both credit and debit transactions. Display the result with the columns \"transaction_date\" and \"total_balance\".\n",
    "\n",
    "# 2. Given three tables: Employees, Departments, and Projects, write a SQL query to find the employees who have been involved in at least two projects from different departments. \n",
    "# The query should return the employee's name and the number of distinct departments they have been involved with.\n",
    "\n",
    "# ğƒğšğ­ğš ğ–ğšğ«ğğ¡ğ¨ğ®ğ¬ğ¢ğ§ğ  ğ‚ğ¨ğ§ğœğğ©ğ­ğ¬:\n",
    "\n",
    "# 3. Explain the difference between a Star Schema and a Snowflake Schema, including their respective structures and when one would be preferred over the other.\n",
    "\n",
    "# 4. In a Star Schema, how would you handle slowly changing dimensions? Discuss the different types of slowly changing dimensions (Type 1, Type 2, Type 3) and techniques for managing them.\n",
    "\n",
    "# 5. Why did you prefer the Snowflake Schema in your project? What were the key components you kept in mind while designing a Snowflake Schema?\n",
    "\n",
    "# ğğ²ğ’ğ©ğšğ«ğ¤-ğ«ğğ¥ğšğ­ğğ ğğ®ğğ¬ğ­ğ¢ğ¨ğ§ğ¬:\n",
    "\n",
    "# 6. Explain how the Spark architecture works internally. Why is Spark faster than MapReduce?\n",
    "\n",
    "# 7. Write code to implement incremental load in PySpark.\n",
    "\n",
    "# 8. What optimizations have you used in your previous project? What is salting?\n",
    "\n",
    "# 9. Why should we not use wide transformations? Give some scenarios where you can eliminate the effects of using wide transformations.\n",
    "\n",
    "# 10. You are given a PySpark DataFrame `transactions` representing e-commerce transactions with a schema containing columns like order_date, customer_id, product_id, quantity, and price. Write PySpark code to perform the following tasks:\n",
    "#  - Calculate the total revenue generated from all orders.\n",
    "#  - Find the top 10 customers by total revenue.\n",
    "#  - For each product, calculate the average price and the total quantity sold.\n",
    "#  - Identify the products that have been sold in all months of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume Crafting: Iâ€™ve updated my resume to show my projects, skills, and achievements, including my experience with data platforms and cloud pipelines.\n",
    "# Skill Enhancement: Iâ€™m focusing on problem-solving with SQL, Python, and PySpark by practising mock interviews with my mentor.\n",
    "# Job Searching: Iâ€™ve updated my profile on Naukri, LinkedIn, and Instahyre, and also reaching out to my connections for referrals to maximize my opportunities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have two tables, \n",
    "# ğŸš€ table A \n",
    "# 1\n",
    "# 1\n",
    "# 1\n",
    "# 0\n",
    "\n",
    "# ğŸš€ Table B\n",
    "# 1\n",
    "# 1\n",
    "# 0\n",
    "# 1\n",
    "# Null\n",
    "\n",
    "# â­• Now tell me the count for Left Join, Right Join, Inner Join, Full Join?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PwC:\n",
    "# Generally there were 3 Rounds of Interviews --\n",
    "\n",
    "# Round 1 --\n",
    "# Very crucial and nervous\n",
    "\n",
    "# Current Project Explanation.\n",
    "# Any Major issues resolved in the current project.\n",
    "# What all optimization techniques have you used in your project\n",
    "# Hadoop Architecture\n",
    "# Three SQL questions majorly on joins, subqueries, Group By, inline view,with Clause, timestamp.\n",
    "# Coding Questions Pyspark or scala Spark.\n",
    "# What are different constraints in sql and why do they different?\n",
    "\n",
    "# Round 2 --\n",
    "# Mostly focused on Scenario based questions. Those include--\n",
    "\n",
    "# How do you increase mappers?\n",
    "# What if Running Job fails in sqoop?\n",
    "# How do you update with Latest data on Sqoop?\n",
    "# What you do if data skewing happens?\n",
    "# What kind of file formats do you use? and where?\n",
    "# Why we need RDD?\n",
    "# What is the use driver in spark?\n",
    "# How do you tackle Memory exceptions errors in spark?\n",
    "# What kind of join do you use if one partition has more data and others have less data?\n",
    "# Why do we need to use containers in spark?\n",
    "# What happens if we increase More partitions in spark?\n",
    "# Write down command to increase no.of partitions?\n",
    "# Write down UDF query?\n",
    "\n",
    "# Round 3 --\n",
    "# Important to get through or rejected\n",
    "\n",
    "# Architecture of MapReduce?\n",
    "# What is Outliers in MapReduce?\n",
    "# What is Partition, shuffle & sort?\n",
    "# What is block report in Hadoop?\n",
    "# Partition By Vs Bucketing in Hive?\n",
    "# Difference between Cassandra and HBase?\n",
    "# What is Catalyst optimizer?\n",
    "# Explain the ETL tools have you used?\n",
    "# Explain basic cloud concepts and more questions on specific cloud we worked?\n",
    "# DataFrame Vs Dataset?\n",
    "# Explain Broadcast join?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL:\n",
    "# Explain the various types of Joins?\n",
    "# Explain Equi join?\n",
    "# Explain the Right Outer Join?\n",
    "# How do you alter the name of a column?\n",
    "# how can you build a stored procedure?\n",
    "# Distinguish between MongoDB and MySQL?\n",
    "# Compare the 'Having' and 'Where' clauses in detail?\n",
    "# What are the differences between COALESCE() and ISNULL()?\n",
    "# What is the difference between â€œStored Procedureâ€ and â€œFunctionâ€?\n",
    "# What is the difference between the â€œDELETEâ€ and â€œTRUNCATEâ€ commands?\n",
    "# What is difference between â€œClustered Indexâ€ and â€œNon Clustered Indexâ€?\n",
    "# What is the difference between â€œPrimary Keyâ€ and â€œUnique Keyâ€?\n",
    "# What is the difference between a â€œLocal Temporary Tableâ€ and â€œGlobal Temporary Tableâ€?\n",
    "# What is the difference between primary key and unique constraints?\n",
    "# What are the differences between DDL, DML and DCL in SQL?\n",
    "# What is a view in SQL? How to create view?\n",
    "# What is a Trigger?\n",
    "# What is the difference between Trigger and Stored Procedure?\n",
    "# What are indexes?\n",
    "# What are Primary Keys and Foreign Keys?\n",
    "# What are wildcards used in database for Pattern Matching?\n",
    "# What is Union, minus and Interact commands?\n",
    "# What is RDBMS?\n",
    "# What is OLTP?\n",
    "# What is Aggregate Functions?\n",
    "# What is the difference between UNION and UNION ALL?\n",
    "# What is a foreign key, and what is it used for?\n",
    "\n",
    "# Scenario Based Questions--\n",
    "#  SQL Query to find second highest salary of Employee?\n",
    "#  SQL Query to find Max Salary from each department?\n",
    "#  Write SQL Query to display current date?\n",
    "#  Write an SQL Query to check whether date passed to Query is date of given format or not?\n",
    "#  Write a SQL Query to print the name of distinct employee whose DOB is between 01/01/1960 to 31/12/1975?\n",
    "#  Write an SQL Query to find employee whose Salary is equal or greater than 10000?\n",
    "#  Write an SQL Query to find name of employee whose name Start with â€˜Mâ€™?\n",
    "#  Find the 3rd MAX salary in the emp table?\n",
    "#  Suppose there is annual salary information provided by emp table. \n",
    "# How to fetch monthly salary of each and every employee?\n",
    "# Display the list of employees who have joined the company before 30th June 90 or after 31st dec 90?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #apachespark is the most discussed topic in all of my interviews. Hence here iam sharing you the interviewquestions have asked in my interview rounds.\n",
    "\n",
    "# Those includes --\n",
    "# What is spark? Explain Architecture\n",
    "# Explain where did you use spark in your project?\n",
    "# What all optimization techniques have you used in spark?\n",
    "# Explain transformations and actions have you used?\n",
    "# What happens when you use shuffle in spark?\n",
    "# Difference between ReduceByKey Vs GroupByKey?\n",
    "# Explain the issues you resolved when you working with spark?\n",
    "# Compare Spark vs Hadoop MapReduce?\n",
    "# Difference between Narrow & wide transformations?\n",
    "# What is partition and how spark Partitions the data?\n",
    "# What is RDD?\n",
    "# what is broadcast variable?\n",
    "# Difference between Sparkcontext Vs Sparksession?\n",
    "# Explain about transformations and actions in the spark?\n",
    "# what is Executor memory in spark?\n",
    "# What is lineage graph?\n",
    "# What is DAG?\n",
    "# Explain libraries that Spark Ecosystem supports?\n",
    "# What is a DStream?\n",
    "# What is Catalyst optimizer and explain it?\n",
    "# Why parquet file format is best for spark?\n",
    "# Difference between dataframe Vs Dataset Vs RDD?\n",
    "# Explain features of Apache Spark?\n",
    "# Explain Lazy evaluation and why is it need?\n",
    "# Explain Pair RDD?\n",
    "# What is Spark Core?\n",
    "# What is the difference between persist() and cache()?\n",
    "# What are the various levels of persistence in Apache Spark?\n",
    "# Does Apache Spark provide check pointing?\n",
    "# How can you achieve high availability in Apache Spark?\n",
    "# Explain Executor Memory in a Spark?\n",
    "# What are the disadvantages of using Apache Spark?\n",
    "# What is the default level of parallelism in apache spark?\n",
    "# Compare map() and flatMap() in Spark?\n",
    "# Difference between repartition Vs coalesce?\n",
    "# Explain Spark Streaming?\n",
    "# Explain accumulators?\n",
    "# What is the use of broadcast join?\n",
    "\n",
    "# What is PySpark Architecture?\n",
    "# What's the difference between an RDD, a DataFrame & DataSet?\n",
    "# How can you create a DataFrame a) using existing RDD, and b) from a CSV file?\n",
    "# Explain the use of StructType and StructField classes in PySpark with examples?\n",
    "# What are the different ways to handle row duplication in a PySpark DataFrame?\n",
    "# Explain PySpark UDF with the help of an example?\n",
    "# Discuss the map() transformation in PySpark DataFrame\n",
    "# what do you mean by â€˜joinsâ€™ in PySpark DataFrame? What are the different types of joins?\n",
    "# What is PySpark ArrayType?\n",
    "# What is PySpark Partition?\n",
    "# What is meant by PySpark MapType? How can you create a MapType using StructType?\n",
    "# How can PySpark DataFrame be converted to Pandas DataFrame?\n",
    "# What is the function of PySpark's pivot() method?\n",
    "# In PySpark, how do you generate broadcast variables?\n",
    "# When to use Client and Cluster modes used for deployment?\n",
    "# How can data transfers be kept to a minimum while using PySpark?\n",
    "# What are Sparse Vectors? What distinguishes them from dense vectors?\n",
    "# What API does PySpark utilize to implement graphs?\n",
    "# What is meant by Piping in PySpark?\n",
    "# What are the various levels of persistence that exist in PySpark?\n",
    "# List some of the benefits of using PySpark?\n",
    "# Why do we use PySpark SparkFiles?\n",
    "# Does PySpark provide a machine learning API?\n",
    "# What are the types of PySparkâ€™s shared variables and why are they useful?\n",
    "# What PySpark DAGScheduler?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common spark interview questions that might help for cracking interviews as well as for better learning!!\n",
    "\n",
    "# Picking up another conceptual topic related to spark which is most frequently asked i.e. :\n",
    "\n",
    "# Off heap vs on heap memory :\n",
    "\n",
    "# Talking about off heap memory :\n",
    "\n",
    "# Each executor within worker node has access to off heap memory and it can be used by spark explicitly for storing its data.\n",
    "# Amount of off heap memory can be governed by spark.memory.offHeap.size and can be enabled by setting spark.memory.offHeap.use to true.\n",
    "\n",
    "# Talking about on heap memory :\n",
    "\n",
    "# On heap memory comprises of three sections:\n",
    "\n",
    "# 1. Reserved memory : Reserved by spark for internal purposes and is equal to 300 mb per executor.\n",
    "\n",
    "# 2. User memory : For storing the data structures created and managed by the user's code and comprises of 40% of on heap memory.\n",
    "\n",
    "# 3. Unified memory : It comprises of 60% of on heap memory.\n",
    "\n",
    "\n",
    "# Unified memory = Execution memory + storage memory \n",
    "\n",
    "# Execution memory : JVM heap space used by data structures during shuffle operations(join and aggregations)\n",
    "\n",
    "# Storage memory : JVM heap space reserved for cached data \n",
    "\n",
    "# To summarise , enabling off heap memory avoids Garbage collection scan overhead and accessing off heap is slightly slower than accessing on heap storage but still faster than reading or writing from a disk because of serialization /deserialization overload .On heap memory is managed and controlled by Garbage collector where as off heap memory is manged by OS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common spark interview questions that might help for cracking interviews as well as for better learning!!\n",
    "\n",
    "# Picking up another theoretical topic related to spark which is most frequently asked i.e. :\n",
    "\n",
    "# Difference between parquet , avro and ORC file formats supported by pyspark.\n",
    "\n",
    "# Parquet:\n",
    "# Parquet is a columnar storage format developed by the Apache Software Foundation for use in the Hadoop ecosystem.\n",
    "# It is optimized for query performance and efficient storage of data.\n",
    "# Parquet stores data in a columnar format, meaning that values from the same column are stored together, allowing for better compression (\"snappy\", \"gzip\", \"lzo\", \"brotli\", or \"zstd\") and faster query processing, especially when dealing with selective column projections.\n",
    "# It supports nested data structures, making it suitable for handling complex data types like arrays and structs.\n",
    "# Parquet files are splittable, meaning they can be read in parallel, which is crucial for distributed processing frameworks like Apache Spark.\n",
    "\n",
    "# Exaple to save DataFrame to Parquet with Snappy compression:\n",
    "# df.write.parquet(\"/path/to/save/location\", compression=\"snappy\")\n",
    "\n",
    "# Avro:\n",
    "# Avro is a row-based data serialization system developed within the Apache Hadoop project.\n",
    "# It is designed to be compact, fast, and suitable for both serialization and data exchange between systems.\n",
    "# Avro supports schema evolution, meaning that you can change the schema of your data without needing to modify the entire dataset.\n",
    "# Avro supports rich data structures, including nested records and complex types like arrays and maps.\n",
    "# It provides features like data compression ( \"snappy\", \"gzip\", \"deflate\", \"bzip2\", or \"zstandard\") and schema resolution, making it efficient for use cases like data serialization and data exchange between different systems.\n",
    "# Avro is self-describing, meaning that the schema is stored along with the data, making it easier to read and process without prior knowledge of the schema.\n",
    "\n",
    "# Example to save DataFrame to Avro with Snappy compression:\n",
    "# df.write.format(\"avro\").option(\"compression\", \"snappy\").save(\"/path/to/save/location\")\n",
    "\n",
    "\n",
    "\n",
    "# ORC (Optimized Row Columnar):\n",
    "# ORC is a columnar storage format developed for use in the Apache Hive data warehouse system.\n",
    "# It is optimized for both read and write performance, especially in the context of analytics and data warehousing workloads.\n",
    "# ORC provides features like predicate pushdown and column projections, allowing for efficient query processing by reading only the necessary columns and rows.\n",
    "# Similar to Parquet, ORC files are splittable and support compression (\"snappy\", \"zlib\", \"lzo\", \"lz4\", or \"zstd\"), making them suitable for use in distributed processing frameworks like Apache Spark.\n",
    "# ORC also supports schema evolution, enabling you to evolve your data schema over time without requiring a full data rewrite.\n",
    "\n",
    "# Example to save DataFrame to ORC with Snappy compression:\n",
    "# df.write.orc(\"/path/to/save/location\", compression=\"snappy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common spark interview questions that might help for cracking interviews as well as for better learning!!\n",
    "\n",
    "# Another rarely asked question is UDF in pyspark:\n",
    "\n",
    "# PySpark UDF is a User Defined Function that is used to create a reusable function in Spark.\n",
    "\n",
    "# CREATION OF UDF IN PYSPARK:\n",
    "# i. Creating UDF ,first we write python function :\n",
    "\n",
    "# def upperCase(str):\n",
    "#  return str.upper()\n",
    "\n",
    "# ii. Then Converting function to UDF using below code:\n",
    "\n",
    "# upperCaseUDF = udf(lambda z: upperCase(z))\n",
    "\n",
    "# iii. Then directly using it with dataframes as below:\n",
    "\n",
    "# df.withColumn(\"Cureated Name\", upperCaseUDF(col(\"Name\"))) \\\n",
    "#  .show(truncate=False)\n",
    "\n",
    "# CREATION OF UDF USING SPARK SQL:\n",
    "# i. In order to use above defined upperCase() function on PySpark SQL, you need to register the function with PySpark by using spark.udf.register().\n",
    "\n",
    "# spark.udf.register(\"upperCaseUDF\", upperCase,StringType())\n",
    "# df.createOrReplaceTempView(\"NAME_TABLE\")\n",
    "# spark.sql(\"select Seqno, upperCaseUDF(Name) as Name from NAME_TABLE\").show(truncate=False)\n",
    "\n",
    "# CREATION OF UDF USING ANNOTATIONS:\n",
    "# i. Create it with just a single step by using annotations.\n",
    "\n",
    "# @udf(returnType=StringType()) \n",
    "# def upperCase(str):\n",
    "#  return str.upper()\n",
    "\n",
    "# df.withColumn(\"Cureated Name\", upperCase(col(\"Name\"))) \\\n",
    "# .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common spark interview questions that might help for cracking interviews as well as for better learning!!\n",
    "\n",
    "# Picking up a practical question related to spark i.e. :\n",
    "\n",
    "# How to dynamically rename multiple columns in PySpark dataFrame ?\n",
    "\n",
    "# Methods to dynamically rename multiple columns in PySpark data frame:\n",
    "# i. Using loops :\n",
    "# First obtain all the columns in the list using the columns function: \n",
    "\n",
    "# total_columns=data_frame.columns\n",
    "\n",
    "# Further, run a loop to dynamically rename multiple columns in Pyspark data frame using prefix, suffix or doing any other changes:\n",
    "\n",
    "# for i in range(len(total_columns)):\n",
    "#  data_frame=data_frame.withColumnRenamed(total_columns[i], 'class_'+ total_columns[i])\n",
    "#  hashtag#or\n",
    "#  data_frame=data_frame.withColumnRenamed(total_columns[i], total_columns[i].replace('_','__'))\n",
    "\n",
    "# ii. Using reduce() function :\n",
    "# Dynamically rename multiple columns in PySpark data frame using prefix, suffix or doing any other changes using reduce() function:\n",
    "\n",
    "# import functools\n",
    "# data_frame = functools.reduce(lambda data_frame,\n",
    "#  idx: data_frame.withColumnRenamed(list(data_frame.schema.names)[idx],\n",
    "#  list(data_frame.schema.names)[idx] + '_suffix'),\n",
    "#  range(len(list(data_frame.schema.names))), data_frame)\n",
    "\n",
    "# Later on, dynamically rename multiple columns in PySpark data frame by replacing some characters using replace and reduce() function:\n",
    "\n",
    "# data_frame = functools.reduce(lambda data_frame,\n",
    "#  idx: data_frame.withColumnRenamed(list(data_frame.schema.names)[idx],\n",
    "#  list(data_frame.schema.names)[idx].replace('hashtag#character','hashtag#other-character')),\n",
    "#  range(len(list(data_frame.schema.names))), data_frame)\n",
    "\n",
    "# iii. Using the alias() function:\n",
    "# Dynamically rename multiple columns in PySpark data frame using prefix, suffix or doing any other changes using alias:\n",
    "\n",
    "# updated_columns = [col(col_name).alias(\"prefix_\" + col_name + \"_suffix\") for col_name in data_frame.columns]\n",
    "\n",
    "#  v. Using toDF() function:\n",
    "# Define the new column names which you want to give to all the columns:\n",
    "\n",
    "# columns=['new_column_name_1','new_column_name_2','new_column_name_3']\n",
    "\n",
    "# Finally, use the function toDF() and assign the names to the data frame and display it:\n",
    "\n",
    "# data_frame.toDF(*columns).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark Data Engineer Interview experience at Big 4 - KPMG India Deloitte EY PwC (4 years of experience)\n",
    "\n",
    "# ğˆğ§ğ­ğ«ğ¨ğğ®ğœğ­ğ¢ğ¨ğ§:\n",
    "# 1. Can you provide an overview of your experience working with PySpark and big data processing?\n",
    "# 2. What motivated you to specialize in PySpark, and how have you applied it in your previous roles?\n",
    "\n",
    "# ğğ²ğ’ğ©ğšğ«ğ¤ ğğšğ¬ğ¢ğœğ¬:\n",
    "# 3. Explain the basic architecture of PySpark.\n",
    "# 4. How does PySpark relate to Apache Spark, and what advantages does it offer in distributed data processing?\n",
    "\n",
    "# ğƒğšğ­ğšğ…ğ«ğšğ¦ğ ğğ©ğğ«ğšğ­ğ¢ğ¨ğ§ğ¬:\n",
    "# 5. Describe the difference between a DataFrame and an RDD in PySpark.\n",
    "# 6. Can you explain transformations and actions in PySpark DataFrames?\n",
    "# 7. Provide examples of PySpark DataFrame operations you frequently use.\n",
    "\n",
    "# ğğ©ğ­ğ¢ğ¦ğ¢ğ³ğ¢ğ§ğ  ğğ²ğ’ğ©ğšğ«ğ¤ ğ‰ğ¨ğ›ğ¬:\n",
    "# 8. How do you optimize the performance of PySpark jobs?\n",
    "# 9. Can you discuss techniques for handling skewed data in PySpark?\n",
    "\n",
    "# ğƒğšğ­ğš ğ’ğğ«ğ¢ğšğ¥ğ¢ğ³ğšğ­ğ¢ğ¨ğ§ ğšğ§ğ ğ‚ğ¨ğ¦ğ©ğ«ğğ¬ğ¬ğ¢ğ¨ğ§:\n",
    "# 10. Explain how data serialization works in PySpark.\n",
    "# 11. Discuss the significance of choosing the right compression codec for your PySpark applications.\n",
    "\n",
    "# ğ‡ğšğ§ğğ¥ğ¢ğ§ğ  ğŒğ¢ğ¬ğ¬ğ¢ğ§ğ  ğƒğšğ­ğš:\n",
    "# 12. How do you deal with missing or null values in PySpark DataFrames?\n",
    "# 13. Are there any specific strategies or functions you prefer for handling missing data?\n",
    "\n",
    "# ğ–ğ¨ğ«ğ¤ğ¢ğ§ğ  ğ°ğ¢ğ­ğ¡ ğğ²ğ’ğ©ğšğ«ğ¤ ğ’ğğ‹:\n",
    "# 14. Describe your experience with PySpark SQL.\n",
    "# 15. How do you execute SQL queries on PySpark DataFrames?\n",
    "\n",
    "# ğğ«ğ¨ğšğğœğšğ¬ğ­ğ¢ğ§ğ  ğ¢ğ§ ğğ²ğ’ğ©ğšğ«ğ¤:\n",
    "# 16. What is broadcasting, and how is it useful in PySpark?\n",
    "# 17. Provide an example scenario where broadcasting can significantly improve performance.\n",
    "\n",
    "# ğğ²ğ’ğ©ğšğ«ğ¤ ğŒğšğœğ¡ğ¢ğ§ğ ğ‹ğğšğ«ğ§ğ¢ğ§ğ :\n",
    "# 18. Discuss your experience with PySpark's MLlib.\n",
    "# 19. Can you give examples of machine learning algorithms you've implemented using PySpark?\n",
    "\n",
    "# ğ‰ğ¨ğ› ğŒğ¨ğ§ğ¢ğ­ğ¨ğ«ğ¢ğ§ğ  ğšğ§ğ ğ‹ğ¨ğ ğ ğ¢ğ§ğ :\n",
    "# 20. How do you monitor and troubleshoot PySpark jobs?\n",
    "# 21. Describe the importance of logging in PySpark applications.\n",
    "\n",
    "# ğˆğ§ğ­ğğ ğ«ğšğ­ğ¢ğ¨ğ§ ğ°ğ¢ğ­ğ¡ ğğ­ğ¡ğğ« ğ“ğğœğ¡ğ§ğ¨ğ¥ğ¨ğ ğ¢ğğ¬:\n",
    "# 22. Have you integrated PySpark with other big data technologies or databases? If so, please provide examples.\n",
    "# 23. How do you handle data transfer between PySpark and external systems?\n",
    "\n",
    "# ğ‘ğğšğ¥-ğ°ğ¨ğ«ğ¥ğ ğğ«ğ¨ğ£ğğœğ­ ğ’ğœğğ§ğšğ«ğ¢ğ¨:\n",
    "# 24. Explain the project that you worked on in your previous organizations.\n",
    "# 25. Describe a challenging PySpark project you've worked on. What were the key challenges, and how did you overcome them?\n",
    "\n",
    "# ğ‚ğ¥ğ®ğ¬ğ­ğğ« ğŒğšğ§ğšğ ğğ¦ğğ§ğ­:\n",
    "# 26. Explain your experience with cluster management in PySpark.\n",
    "# 27. How do you scale PySpark applications in a cluster environment?\n",
    "\n",
    "# ğğ²ğ’ğ©ğšğ«ğ¤ ğ„ğœğ¨ğ¬ğ²ğ¬ğ­ğğ¦:\n",
    "# 28. Can you name and briefly describe some popular libraries or tools in the PySpark ecosystem, apart from the core PySpark functionality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğ‘ğğšğ¥-ğ“ğ¢ğ¦ğ ğ’ğ©ğšğ«ğ¤ ğ’ğœğğ§ğšğ«ğ¢ğ¨ ğˆğ§ğ­ğğ«ğ¯ğ¢ğğ° ğğ®ğğ¬ğ­ğ¢ğ¨ğ§:\n",
    "\n",
    "# ğğ®ğğ¬ğ­ğ¢ğ¨ğ§:\n",
    "# You have a clickstream table consisting of >10 billion rows, you wanted to write a sample query to fetch details of customers who clicked on a recently launched campaign. As the data is really huge its advised to run the query on the sample data first, how do you handle this situation and extract only a small percent of data to run your query.\n",
    "\n",
    "# ğ’ğ¨ğ¥ğ®ğ­ğ¢ğ¨ğ§:\n",
    "# TABLESAMPLE x PERCENT \n",
    "# It is used to randomly select a sampling of data from a table. \"x\" specifies the percentage of data that will be included in the sample.\n",
    "\n",
    "# It does not guarantee an exact percentage of rows to be selected as a sample. This is because the sample is selected randomly, so the actual percentage of selected rows may vary slightly.\n",
    "\n",
    "# ğ’ğšğ¦ğ©ğ¥ğ. ğğ®ğğ«ğ²:\n",
    "# SELECT \n",
    "#  customer_id, click_time, product_id\n",
    "# FROM clickstream_table TABLESAMPLE (1 PERCENT)\n",
    "# WHERE campaign_id = 12345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Interview_Questions_for_Data_Engineer\n",
    "# **Question 26: What are some best practices for tuning Spark applications for optimal performance?**\n",
    "\n",
    "# Tuning Spark applications for optimal performance is essential to make the most of your cluster's resources. Here are some best practices for achieving this and an example code sample for each practice:\n",
    "# ğŸ”¹Memory Management:\n",
    "# Optimize memory allocation: Set the appropriate memory configurations, such as spark.driver.memory, spark.executor.memory, and spark.memory.fraction, to efficiently use available resources.\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#  .appName(\"MemoryManagementExample\") \\\n",
    "#  .config(\"spark.driver.memory\", \"2g\") \\\n",
    "#  .config(\"spark.executor.memory\", \"4g\") \\\n",
    "#  .getOrCreate()\n",
    "\n",
    "# spark.stop()\n",
    "\n",
    "# ğŸ”¹Data Serialization:\n",
    "# Choose efficient serialization: Use a more efficient serialization format like Avro or Parquet, which can reduce the amount of data transfer and storage.\n",
    "\n",
    "# spark.conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "# spark.conf.set(\"spark.kryo.registrator\", \"your.package.YourKryoRegistrator\")\n",
    "\n",
    "# ğŸ”¹Data Partitioning:\n",
    "# Optimize data partitioning: Repartition your data using repartition or coalesce to balance data distribution and avoid data skew.\n",
    "\n",
    "# df = df.repartition(100)\n",
    "\n",
    "# ğŸ”¹Caching and Persistence:\n",
    "# Cache and persist data: Use cache() or persist() to store frequently used data in memory for faster access.\n",
    "\n",
    "# df = df.cache()\n",
    "\n",
    "# ğŸ”¹Shuffling Optimization:\n",
    "# Minimize shuffling: Avoid unnecessary shuffling by using operations like reduceByKey instead of groupByKey.\n",
    "\n",
    "# rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# ğŸ”¹Broadcasting:\n",
    "# Broadcast small data: Broadcast small DataFrames to all nodes to avoid unnecessary data transfer.\n",
    "\n",
    "# broadcast_small_df = spark.sparkContext.broadcast(small_df)\n",
    "\n",
    "# ğŸ”¹Optimized Joins:\n",
    "# Use optimized join strategies: Utilize broadcast join, bucketed join, or sort-merge join based on your data distribution.\n",
    "\n",
    "# joined_df = large_df.join(broadcast(small_df), \"id\", \"inner\")\n",
    "\n",
    "# ğŸ”¹Dynamic Resource Allocation:\n",
    "# Enable dynamic allocation: Allow Spark to allocate and release resources dynamically based on the workload.\n",
    "# spark.conf.set(\"spark.dynamicAllocation.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/DataExpert-io/data-engineer-handbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apache Spark has a distributed and modular internal architecture that allows it to efficiently process large volumes of data. Below, I'll provide an overview of Spark's internal architecture:\n",
    "\n",
    "# 1. **Driver Program:**\n",
    "#  - The Driver Program is the entry point of any Spark application. It runs the `main()` function and creates a SparkSession or SparkContext, which is used to coordinate the execution of Spark tasks.\n",
    "#  - The Driver Program is responsible for dividing the application into tasks, scheduling these tasks on the cluster, and managing their execution.\n",
    "\n",
    "# 2. **Cluster Manager:**\n",
    "#  - Spark can run on various cluster managers, including Apache Hadoop YARN, Apache Mesos, and its standalone cluster manager.\n",
    "#  - The cluster manager allocates resources (CPU and memory) to Spark applications and monitors their progress.\n",
    "\n",
    "# 3. **Cluster Nodes (Executors):**\n",
    "#  - Executors are worker nodes in the Spark cluster where the actual computation takes place.\n",
    "#  - Each executor runs tasks in separate JVM processes. Executors communicate with the Driver Program and with each other for data shuffling and task coordination.\n",
    "\n",
    "# 4. **Task:**\n",
    "#  - A task is the smallest unit of work in Spark and represents a computation to be executed on a single partition of data.\n",
    "#  - Tasks are spawned by the Driver Program and run on the Executors.\n",
    "\n",
    "# 5. **Distributed Data Storage:**\n",
    "#  - Spark can use distributed data storage systems like Hadoop Distributed File System (HDFS) and Apache HBase to store and access data.\n",
    "#  - Data is divided into partitions and distributed across the cluster, enabling parallel processing.\n",
    "\n",
    "# 6. **Resilient Distributed Dataset (RDD):**\n",
    "#  - RDD is Spark's fundamental data structure. It represents distributed collections of data that can be processed in parallel.\n",
    "#  - RDDs are immutable and fault-tolerant, meaning they can recover from node failures by recomputing lost data.\n",
    "\n",
    "# 7. **Directed Acyclic Graph (DAG):**\n",
    "#  - Spark uses a DAG to represent a logical execution plan of tasks and transformations.\n",
    "#  - Each stage in the DAG corresponds to a set of transformations that can be executed in parallel.\n",
    "#  - The DAG scheduler optimizes the execution plan and schedules tasks based on data dependencies.\n",
    "\n",
    "# 8. **Transformations and Actions:**\n",
    "#  - Transformations are operations applied to RDDs to create new RDDs (e.g., `map`, `filter`, `reduceByKey`).\n",
    "#  - Actions are operations that trigger the execution of transformations and return results (e.g., `collect`, `count`, `saveAsTextFile`).\n",
    "\n",
    "# 9. **Shuffling:**\n",
    "#  - Shuffling is the process of redistributing data across partitions or nodes in the cluster.\n",
    "#  - It's a costly operation and can impact performance, so minimizing shuffling is important for Spark optimization.\n",
    "\n",
    "# 10. **Caching:**\n",
    "#  - Spark allows you to cache (persist) RDDs or DataFrames in memory, improving performance for iterative algorithms and interactive data analysis.\n",
    "\n",
    "# #apachespark #pyspark #dataengineering #dataengineer #bigdataengineer #bigdatadeveloper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 trending questions asked in Apache Spark interviews\n",
    "\n",
    "# 1. how are initial number of partitions calculated in a dataframe\n",
    "\n",
    "# 2. what happens internally when you execute spark-submit\n",
    "\n",
    "# 3. what is a partition skew and how to tackle it\n",
    "\n",
    "# 4. what are the spark optimization techniques you have used\n",
    "\n",
    "# 5. what is a broadcast join, how does it work internally\n",
    "\n",
    "# 6. how do you optimize 2 large table joins\n",
    "\n",
    "# 7. please explain about memory management in apache spark\n",
    "\n",
    "# 8. what is caching in spark, and when do you consider caching a dataframe\n",
    "\n",
    "# 9. how do you handle out of memory errors in spark\n",
    "\n",
    "# 10. what is the difference between partitioning and bucketing, please explain with a usecase\n",
    "\n",
    "# Do mention you answers in the comments!\n",
    "\n",
    "# what other questions in apache spark are trending?\n",
    "\n",
    "# P.S ~ I teach big data and my students are leading big data teams in top companies. My new batch is starting on coming Saturday.\n",
    "# visit my website https://lnkd.in/gt_jpCyE to know more.\n",
    "\n",
    "# #bigdata #dataengineering #apachespark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to choose the number of Executor cores in Apache Spark?\n",
    "\n",
    "# We struggle to decide on the Spark configurations and mostly rely on default ones. I agree that in 95 % of scenarios the default configuration is the most optimized one. But sometimes we also need to tune the configuration parameters for performance and so it should not be a black box for us. \n",
    "\n",
    "# Let's try to understand how to decide on the number of executor cores and get a framework for it. Before that keep the following things in mind:\n",
    "\n",
    "# - Spark application requires daemon processes to run in the background. So set aside 1 core per node for that.\n",
    "# - YARN Application Manager is responsible for negotiating resources with Resource manager. Set aside one executor in the cluster for that.\n",
    "# - A lot of concurrent threads (More than 5) degrade the HDFS throughput.\n",
    "\n",
    "# Cluster Configuration: 10 nodes, 16 cores, 64 GB\n",
    "\n",
    "# There are two extreme ways to configure executors:\n",
    "\n",
    "# 1. Tiny Executors: 1 core per executor, number of executor per node=16, executor-memory=64/16=4GB, total number of executors=10*16=160\n",
    "\n",
    "# Problems: 1. Multithreading is not possible since only one core per executor. 2. When using shared variables like broadcast or accumulator it has to be replicated in all the 16 executors per node- A lot of shuffling\n",
    "\n",
    "\n",
    "# 1. Fat Executors: 16 cores per executor, number of executor per node=1, executor-memory=64/1=64GB, total number of executors=1*10=10\n",
    "\n",
    "# Problems: 1. HDFS throughput suffers because of a lot concurrent threading 2. Garbage Collection takes a lot time since memory per executor is very high\n",
    "\n",
    "# Balanced Approach: Let's take 5 cores per executors. Why 5? Because beyond 5 HDFS throughput will suffer and we get good multithreading also.\n",
    "# Leave 1 core per node for daemon processes, Number of cores available per node =16-1=15\n",
    "# Total number of available cores=15*10=150\n",
    "# Number of executors=150/5=30\n",
    "# Leave 1 executor for AM, num-executors=30-1=29\n",
    "# number of executors per node=30/10=3\n",
    "# Memory per executor= 64/3 ~ 21GB\n",
    "# Counting the 7% off heap memory actual Executor memory = 21*93% ~ 19GB\n",
    "\n",
    "# Optimized Config: 29 Executors, 19GB memory, 5 cores each\n",
    "\n",
    "# In practice one size does not fit all. You need to keep tuning as per cluster configuration. But in general the number of executor cores should be 2-5.\n",
    "\n",
    "# I have created a document for the same. Do have a look.\n",
    "\n",
    "# If my posts help you somehow, please follow Shuvajit Hazra and share the post.\n",
    "\n",
    "# #Analytics #DataEngineering #ApacheSpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Architecture Quick Overview\n",
    "\n",
    "# Cluster \n",
    "# ======\n",
    "# A Cluster is a group of JVMs (nodes) connected by the network, each of which runs Spark, either in Driver or Worker roles.\n",
    "\n",
    "# Driver\n",
    "# ======\n",
    "# The Driver is one of the nodes in the Cluster.\n",
    "\n",
    "# The driver does not run computations (filter, map, reduce, etc).\n",
    "# It plays the role of a master node in the Spark cluster.\n",
    "\n",
    "# When you call collect() on an RDD or Dataset, the whole data is sent to the Driver. This is why you should be careful when calling collect().\n",
    "\n",
    "# Executor\n",
    "# ========\n",
    "# Executors are JVMs that run on Worker nodes. These are the JVMs that actually run Tasks on data Partitions.\n",
    "\n",
    "# Application\n",
    "# =========\n",
    "# An application comprises of several jobs. A job is created, whenever you execute an action function like write().\n",
    "\n",
    "# Job\n",
    "# ====\n",
    "# A job comprises of several stages. When Spark encounters a function that requires a shuffle it creates a new stage. \n",
    "\n",
    "# Transformation functions like reduceByKey(), Join() etc will trigger a shuffle and will result in a new stage. \n",
    "\n",
    "# Stage\n",
    "# =====\n",
    "# A stage comprises of several tasks and every task in the stage executes the same set of instructions.\n",
    "\n",
    "# Task\n",
    "# =====\n",
    "# Task is the smallest execution unit in Spark. It is a single operation applied to single partition.\n",
    " \n",
    "# Tasks are executed inside an executor. \n",
    "\n",
    "# A Spark application can have many jobs. \n",
    "# A job can have many stages. \n",
    "# A stage can have many tasks. \n",
    "# A task executes a series of instructions.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
